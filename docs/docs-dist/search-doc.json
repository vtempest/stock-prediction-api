{"searchDocs":[{"title":"dist","type":0,"sectionRef":"#","url":"/functions/api-client/dist","content":"","keywords":"","version":"Next"},{"title":"ClientOptions‚Äã","type":1,"pageTitle":"dist","url":"/functions/api-client/dist#clientoptions","content":" type ClientOptions = object;   Defined in: api-client/dist/index.d.ts:6  ","version":"Next","tagName":"h2"},{"title":"Properties‚Äã","type":1,"pageTitle":"dist","url":"/functions/api-client/dist#properties","content":" baseUrl‚Äã  baseUrl: &quot;http://localhost:8000&quot; | string &amp; object;   Defined in: api-client/dist/index.d.ts:7    ","version":"Next","tagName":"h3"},{"title":"FeatureConfig‚Äã","type":1,"pageTitle":"dist","url":"/functions/api-client/dist#featureconfig","content":" type FeatureConfig = object;   Defined in: api-client/dist/index.d.ts:14  Feature Configuration Configuration object for feature engineering and model training parameters  ","version":"Next","tagName":"h2"},{"title":"Properties‚Äã","type":1,"pageTitle":"dist","url":"/functions/api-client/dist#properties-1","content":" time_features?‚Äã  optional time_features: boolean;   Defined in: api-client/dist/index.d.ts:19  Time Features Include time-based features such as hour, day, month, seasonality  weather_features?‚Äã  optional weather_features: boolean;   Defined in: api-client/dist/index.d.ts:24  Weather Features Include weather-related features and weather interactions  rolling_features?‚Äã  optional rolling_features: boolean;   Defined in: api-client/dist/index.d.ts:29  Rolling Features Include rolling window statistics (mean, std, min, max)  lag_features?‚Äã  optional lag_features: boolean;   Defined in: api-client/dist/index.d.ts:34  Lag Features Include lagged values of energy consumption  interaction_features?‚Äã  optional interaction_features: boolean;   Defined in: api-client/dist/index.d.ts:39  Interaction Features Include feature interactions (e.g., temperature * humidity)  windows?‚Äã  optional windows: number[];   Defined in: api-client/dist/index.d.ts:44  Rolling Windows List of rolling window sizes in days for statistical features  lags?‚Äã  optional lags: number[];   Defined in: api-client/dist/index.d.ts:49  Lag Periods List of lag periods in days for historical energy values    ","version":"Next","tagName":"h3"},{"title":"GetDocumentationData‚Äã","type":1,"pageTitle":"dist","url":"/functions/api-client/dist#getdocumentationdata","content":" type GetDocumentationData = object;   Defined in: api-client/dist/index.d.ts:58  ","version":"Next","tagName":"h2"},{"title":"Properties‚Äã","type":1,"pageTitle":"dist","url":"/functions/api-client/dist#properties-2","content":" body?‚Äã  optional body: never;   Defined in: api-client/dist/index.d.ts:59  path?‚Äã  optional path: never;   Defined in: api-client/dist/index.d.ts:60  query?‚Äã  optional query: never;   Defined in: api-client/dist/index.d.ts:61  url‚Äã  url: &quot;/docs&quot;;   Defined in: api-client/dist/index.d.ts:62    ","version":"Next","tagName":"h3"},{"title":"GetDocumentationResponse‚Äã","type":1,"pageTitle":"dist","url":"/functions/api-client/dist#getdocumentationresponse","content":" type GetDocumentationResponse = GetDocumentationResponses[keyof GetDocumentationResponses];   Defined in: api-client/dist/index.d.ts:65    ","version":"Next","tagName":"h2"},{"title":"GetDocumentationResponses‚Äã","type":1,"pageTitle":"dist","url":"/functions/api-client/dist#getdocumentationresponses","content":" type GetDocumentationResponses = object;   Defined in: api-client/dist/index.d.ts:67  ","version":"Next","tagName":"h2"},{"title":"Properties‚Äã","type":1,"pageTitle":"dist","url":"/functions/api-client/dist#properties-3","content":" 200‚Äã  200: string;   Defined in: api-client/dist/index.d.ts:71  API documentation page    ","version":"Next","tagName":"h3"},{"title":"HttpError‚Äã","type":1,"pageTitle":"dist","url":"/functions/api-client/dist#httperror","content":" type HttpError = object;   Defined in: api-client/dist/index.d.ts:78  HTTP Error HTTP error response  ","version":"Next","tagName":"h2"},{"title":"Properties‚Äã","type":1,"pageTitle":"dist","url":"/functions/api-client/dist#properties-4","content":" detail‚Äã  detail: string;   Defined in: api-client/dist/index.d.ts:83  Error Detail Detailed error message    ","version":"Next","tagName":"h3"},{"title":"ModelMetrics‚Äã","type":1,"pageTitle":"dist","url":"/functions/api-client/dist#modelmetrics","content":" type ModelMetrics = object;   Defined in: api-client/dist/index.d.ts:90  Model Performance Metrics Performance metrics for a trained model  ","version":"Next","tagName":"h2"},{"title":"Properties‚Äã","type":1,"pageTitle":"dist","url":"/functions/api-client/dist#properties-5","content":" model‚Äã  model: &quot;Prophet&quot; | &quot;Random Forest&quot; | &quot;Ridge Regression&quot; | &quot;Ensemble&quot;;   Defined in: api-client/dist/index.d.ts:95  Model Name Name of the machine learning model  mae‚Äã  mae: number;   Defined in: api-client/dist/index.d.ts:100  Mean Absolute Error Mean Absolute Error in millions of energy units  rmse‚Äã  rmse: number;   Defined in: api-client/dist/index.d.ts:105  Root Mean Square Error Root Mean Square Error in millions of energy units  r2_score?‚Äã  optional r2_score: number | null;   Defined in: api-client/dist/index.d.ts:110  R-squared Score Coefficient of determination (R¬≤) indicating model fit quality  accuracy?‚Äã  optional accuracy: number | null;   Defined in: api-client/dist/index.d.ts:115  Accuracy Percentage Model accuracy as percentage (100 - MAPE)    ","version":"Next","tagName":"h3"},{"title":"Options<TData, ThrowOnError>‚Äã","type":1,"pageTitle":"dist","url":"/functions/api-client/dist#optionstdata-throwonerror","content":" type Options&lt;TData, ThrowOnError&gt; = Options_2&lt;TData, ThrowOnError&gt; &amp; object;   Defined in: api-client/dist/index.d.ts:118  ","version":"Next","tagName":"h2"},{"title":"Type declaration‚Äã","type":1,"pageTitle":"dist","url":"/functions/api-client/dist#type-declaration","content":" Name\tType\tDescription\tDefined in client? Client You can provide a client instance returned by createClient() instead of individual options. This might be also useful if you want to implement a custom client. api-client/dist/index.d.ts:124 meta? Record&lt;string, unknown&gt; You can pass arbitrary values through the meta object. This can be used to access values that aren't defined as part of the SDK function. api-client/dist/index.d.ts:129  ","version":"Next","tagName":"h3"},{"title":"Type Parameters‚Äã","type":1,"pageTitle":"dist","url":"/functions/api-client/dist#type-parameters","content":" Type Parameter\tDefault type TData extends TDataShape TDataShape ThrowOnError extends boolean boolean    ","version":"Next","tagName":"h3"},{"title":"PredictionResponse‚Äã","type":1,"pageTitle":"dist","url":"/functions/api-client/dist#predictionresponse","content":" type PredictionResponse = object;   Defined in: api-client/dist/index.d.ts:136  Prediction Response Complete response containing predictions and model performance metrics  ","version":"Next","tagName":"h2"},{"title":"Properties‚Äã","type":1,"pageTitle":"dist","url":"/functions/api-client/dist#properties-6","content":" predictions‚Äã  predictions: PredictionResult[];   Defined in: api-client/dist/index.d.ts:141  Predictions List of energy predictions for each date  cross_validation_results‚Äã  cross_validation_results: ModelMetrics[];   Defined in: api-client/dist/index.d.ts:146  Cross-Validation Results Performance metrics from cross-validation testing  may_validation_results‚Äã  may_validation_results: ModelMetrics[];   Defined in: api-client/dist/index.d.ts:151  May Validation Results Performance metrics from May validation data  feature_config‚Äã  feature_config: FeatureConfig;   Defined in: api-client/dist/index.d.ts:152    ","version":"Next","tagName":"h3"},{"title":"PredictionResult‚Äã","type":1,"pageTitle":"dist","url":"/functions/api-client/dist#predictionresult","content":" type PredictionResult = object;   Defined in: api-client/dist/index.d.ts:159  Prediction Result Energy prediction result for a single date  ","version":"Next","tagName":"h2"},{"title":"Properties‚Äã","type":1,"pageTitle":"dist","url":"/functions/api-client/dist#properties-7","content":" date‚Äã  date: string;   Defined in: api-client/dist/index.d.ts:164  Date Prediction date in YYYY-MM-DD format  predicted_energy_millions‚Äã  predicted_energy_millions: number;   Defined in: api-client/dist/index.d.ts:169  Final Prediction Final ensemble prediction in millions of energy units  prophet_prediction‚Äã  prophet_prediction: number;   Defined in: api-client/dist/index.d.ts:174  Prophet Prediction Prophet model prediction in millions of energy units  rf_prediction‚Äã  rf_prediction: number;   Defined in: api-client/dist/index.d.ts:179  Random Forest Prediction Random Forest model prediction in millions of energy units  ridge_prediction‚Äã  ridge_prediction: number;   Defined in: api-client/dist/index.d.ts:184  Ridge Regression Prediction Ridge Regression model prediction in millions of energy units  ensemble_prediction‚Äã  ensemble_prediction: number;   Defined in: api-client/dist/index.d.ts:189  Ensemble Prediction Average of all model predictions in millions of energy units  prediction_lower‚Äã  prediction_lower: number;   Defined in: api-client/dist/index.d.ts:194  Lower Confidence Bound Lower bound of prediction confidence interval  prediction_upper‚Äã  prediction_upper: number;   Defined in: api-client/dist/index.d.ts:199  Upper Confidence Bound Upper bound of prediction confidence interval  actual_energy_millions‚Äã  actual_energy_millions: number;   Defined in: api-client/dist/index.d.ts:204  Actual Energy Actual energy consumption in millions of energy units  error‚Äã  error: number;   Defined in: api-client/dist/index.d.ts:209  Prediction Error Difference between actual and predicted values (actual - predicted)  percent_error‚Äã  percent_error: number;   Defined in: api-client/dist/index.d.ts:214  Percentage Error Absolute percentage error  weather_data‚Äã  weather_data: WeatherData;   Defined in: api-client/dist/index.d.ts:215    ","version":"Next","tagName":"h3"},{"title":"PredictStatisticsData‚Äã","type":1,"pageTitle":"dist","url":"/functions/api-client/dist#predictstatisticsdata","content":" type PredictStatisticsData = object;   Defined in: api-client/dist/index.d.ts:224  ","version":"Next","tagName":"h2"},{"title":"Properties‚Äã","type":1,"pageTitle":"dist","url":"/functions/api-client/dist#properties-8","content":" body‚Äã  body: FeatureConfig;   Defined in: api-client/dist/index.d.ts:225  path?‚Äã  optional path: never;   Defined in: api-client/dist/index.d.ts:226  query?‚Äã  optional query: never;   Defined in: api-client/dist/index.d.ts:227  url‚Äã  url: &quot;/predict&quot;;   Defined in: api-client/dist/index.d.ts:228    ","version":"Next","tagName":"h3"},{"title":"PredictStatisticsError‚Äã","type":1,"pageTitle":"dist","url":"/functions/api-client/dist#predictstatisticserror","content":" type PredictStatisticsError = PredictStatisticsErrors[keyof PredictStatisticsErrors];   Defined in: api-client/dist/index.d.ts:231    ","version":"Next","tagName":"h2"},{"title":"PredictStatisticsErrors‚Äã","type":1,"pageTitle":"dist","url":"/functions/api-client/dist#predictstatisticserrors","content":" type PredictStatisticsErrors = object;   Defined in: api-client/dist/index.d.ts:233  ","version":"Next","tagName":"h2"},{"title":"Properties‚Äã","type":1,"pageTitle":"dist","url":"/functions/api-client/dist#properties-9","content":" 422‚Äã  422: ValidationError;   Defined in: api-client/dist/index.d.ts:237  Validation Error  500‚Äã  500: HttpError;   Defined in: api-client/dist/index.d.ts:241  Internal Server Error    ","version":"Next","tagName":"h3"},{"title":"PredictStatisticsResponse‚Äã","type":1,"pageTitle":"dist","url":"/functions/api-client/dist#predictstatisticsresponse","content":" type PredictStatisticsResponse = PredictStatisticsResponses[keyof PredictStatisticsResponses];   Defined in: api-client/dist/index.d.ts:244    ","version":"Next","tagName":"h2"},{"title":"PredictStatisticsResponses‚Äã","type":1,"pageTitle":"dist","url":"/functions/api-client/dist#predictstatisticsresponses","content":" type PredictStatisticsResponses = object;   Defined in: api-client/dist/index.d.ts:246  ","version":"Next","tagName":"h2"},{"title":"Properties‚Äã","type":1,"pageTitle":"dist","url":"/functions/api-client/dist#properties-10","content":" 200‚Äã  200: PredictionResponse;   Defined in: api-client/dist/index.d.ts:250  Successful prediction generation    ","version":"Next","tagName":"h3"},{"title":"ValidationError‚Äã","type":1,"pageTitle":"dist","url":"/functions/api-client/dist#validationerror","content":" type ValidationError = object;   Defined in: api-client/dist/index.d.ts:257  Validation Error Request validation error details  ","version":"Next","tagName":"h2"},{"title":"Properties‚Äã","type":1,"pageTitle":"dist","url":"/functions/api-client/dist#properties-11","content":" detail‚Äã  detail: object[];   Defined in: api-client/dist/index.d.ts:261  Error Details  Name\tType\tDescription\tDefined in loc (string | number)[] Location Location of the validation error api-client/dist/index.d.ts:266 msg string Message Error message api-client/dist/index.d.ts:271 type string Error Type Type of validation error api-client/dist/index.d.ts:276    ","version":"Next","tagName":"h3"},{"title":"WeatherData‚Äã","type":1,"pageTitle":"dist","url":"/functions/api-client/dist#weatherdata","content":" type WeatherData = object;   Defined in: api-client/dist/index.d.ts:284  Weather Data Weather information for a specific date  ","version":"Next","tagName":"h2"},{"title":"Properties‚Äã","type":1,"pageTitle":"dist","url":"/functions/api-client/dist#properties-12","content":" temperature_2m_mean?‚Äã  optional temperature_2m_mean: number | null;   Defined in: api-client/dist/index.d.ts:289  Average Temperature Mean temperature at 2 meters height in Celsius  soil_moisture_0_to_7cm_mean?‚Äã  optional soil_moisture_0_to_7cm_mean: number | null;   Defined in: api-client/dist/index.d.ts:294  Soil Moisture Average soil moisture in top 7cm layer  precipitation_sum?‚Äã  optional precipitation_sum: number | null;   Defined in: api-client/dist/index.d.ts:299  Precipitation Total precipitation in mm  relative_humidity_2m_mean?‚Äã  optional relative_humidity_2m_mean: number | null;   Defined in: api-client/dist/index.d.ts:304  Relative Humidity Average relative humidity at 2 meters height as percentage    ","version":"Next","tagName":"h3"},{"title":"getDocumentation()‚Äã","type":1,"pageTitle":"dist","url":"/functions/api-client/dist#getdocumentation","content":" const getDocumentation: &lt;ThrowOnError&gt;(options?: Options&lt;GetDocumentationData, ThrowOnError&gt;) =&gt; RequestResult&lt;GetDocumentationResponses, unknown, ThrowOnError, &quot;fields&quot;&gt;;   Defined in: api-client/dist/index.d.ts:56  API Documentation Interactive API documentation (Swagger UI)  ","version":"Next","tagName":"h2"},{"title":"Type Parameters‚Äã","type":1,"pageTitle":"dist","url":"/functions/api-client/dist#type-parameters-1","content":" Type Parameter\tDefault type ThrowOnError extends boolean false  ","version":"Next","tagName":"h3"},{"title":"Parameters‚Äã","type":1,"pageTitle":"dist","url":"/functions/api-client/dist#parameters","content":" Parameter\tType options? Options&lt;GetDocumentationData, ThrowOnError&gt;  ","version":"Next","tagName":"h3"},{"title":"Returns‚Äã","type":1,"pageTitle":"dist","url":"/functions/api-client/dist#returns","content":" RequestResult&lt;GetDocumentationResponses, unknown, ThrowOnError, &quot;fields&quot;&gt;    ","version":"Next","tagName":"h3"},{"title":"predictStatistics()‚Äã","type":1,"pageTitle":"dist","url":"/functions/api-client/dist#predictstatistics","content":" const predictStatistics: &lt;ThrowOnError&gt;(options: Options&lt;PredictStatisticsData, ThrowOnError&gt;) =&gt; RequestResult&lt;PredictStatisticsResponses, PredictStatisticsErrors, ThrowOnError, &quot;fields&quot;&gt;;   Defined in: api-client/dist/index.d.ts:222  Generate Statistic Predictions Trains multiple machine learning models (Prophet, Random Forest, Ridge Regression) and generates energy consumption predictions with comprehensive validation metrics. The API performs feature engineering, model training, cross-validation, and final predictions for specified dates.  ","version":"Next","tagName":"h2"},{"title":"Type Parameters‚Äã","type":1,"pageTitle":"dist","url":"/functions/api-client/dist#type-parameters-2","content":" Type Parameter\tDefault type ThrowOnError extends boolean false  ","version":"Next","tagName":"h3"},{"title":"Parameters‚Äã","type":1,"pageTitle":"dist","url":"/functions/api-client/dist#parameters-1","content":" Parameter\tType options Options&lt;PredictStatisticsData, ThrowOnError&gt;  ","version":"Next","tagName":"h3"},{"title":"Returns‚Äã","type":1,"pageTitle":"dist","url":"/functions/api-client/dist#returns-1","content":" RequestResult&lt;PredictStatisticsResponses, PredictStatisticsErrors, ThrowOnError, &quot;fields&quot;&gt; ","version":"Next","tagName":"h3"},{"title":"Energy Prediction System","type":0,"sectionRef":"#","url":"/functions/","content":"","keywords":"","version":"Next"},{"title":"Overview‚Äã","type":1,"pageTitle":"Energy Prediction System","url":"/functions/#overview","content":" This system combines multiple machine learning approaches to predict energy consumption with high accuracy. It uses an ensemble of models including Prophet for time series forecasting, Random Forest, Ridge Regression, and XGBoost for pattern recognition, along with advanced feature engineering and cross-validation techniques.  ","version":"Next","tagName":"h2"},{"title":"Key Features‚Äã","type":1,"pageTitle":"Energy Prediction System","url":"/functions/#key-features","content":" Multi-Model Ensemble: Combines Prophet, Random Forest Ridge Regression, and XGBoostAdvanced Feature Engineering: Creates sophisticated features from temporal and weather dataTime Series Cross-Validation: Proper evaluation using forward-chaining validationWeather Integration: Incorporates weather data for improved predictionsConfidence Intervals: Provides prediction uncertainty estimatesReal-time API: FastAPI endpoint for on-demand predictions  ","version":"Next","tagName":"h2"},{"title":"System Architecture‚Äã","type":1,"pageTitle":"Energy Prediction System","url":"/functions/#system-architecture","content":" ","version":"Next","tagName":"h2"},{"title":"Core Components‚Äã","type":1,"pageTitle":"Energy Prediction System","url":"/functions/#core-components","content":" Data Pipeline: Loads and prepares energy consumption and weather dataFeature Engineering: Creates advanced temporal and weather-based featuresModel Training: Trains multiple ML models with hyperparameter optimizationCross-Validation: Evaluates models using time series-aware validationEnsemble Prediction: Combines predictions from multiple modelsAPI Interface: Serves predictions through RESTful API  ","version":"Next","tagName":"h3"},{"title":"Model Types‚Äã","type":1,"pageTitle":"Energy Prediction System","url":"/functions/#model-types","content":" 1. Prophet Model‚Äã  Purpose: Time series forecasting with seasonality detectionStrengths: Handles trends, seasonality, and holidays automaticallyFeatures: Built-in uncertainty quantification and missing data handling  2. Random Forest‚Äã  Purpose: Non-linear pattern recognitionStrengths: Robust to outliers, handles feature interactions wellConfiguration: Optimized hyperparameters for energy data  3. Ridge Regression‚Äã  Purpose: Linear modeling with regularizationStrengths: Prevents overfitting, interpretable coefficientsFeatures: L2 regularization for stable predictions  4. XGBoost‚Äã  Purpose: Gradient boosting for complex patternsStrengths: High accuracy, handles missing valuesConfiguration: Tuned for energy consumption patterns  ","version":"Next","tagName":"h3"},{"title":"Data Sources‚Äã","type":1,"pageTitle":"Energy Prediction System","url":"/functions/#data-sources","content":" ","version":"Next","tagName":"h2"},{"title":"Energy Data‚Äã","type":1,"pageTitle":"Energy Prediction System","url":"/functions/#energy-data","content":" Historical energy consumption measurementsTemporal resolution: Daily/hourly readingsUnits: Typically in megawatts (MW) or millions of units  ","version":"Next","tagName":"h3"},{"title":"Weather Data‚Äã","type":1,"pageTitle":"Energy Prediction System","url":"/functions/#weather-data","content":" Temperature (2m mean)Soil moisture (0-7cm depth)Precipitation sumRelative humidity (2m mean)Additional meteorological variables  ","version":"Next","tagName":"h3"},{"title":"Official Data‚Äã","type":1,"pageTitle":"Energy Prediction System","url":"/functions/#official-data","content":" Ground truth energy consumption valuesUsed for validation and accuracy assessment  ","version":"Next","tagName":"h3"},{"title":"Feature Engineering‚Äã","type":1,"pageTitle":"Energy Prediction System","url":"/functions/#feature-engineering","content":" The system creates advanced features including:  ","version":"Next","tagName":"h2"},{"title":"Temporal Features‚Äã","type":1,"pageTitle":"Energy Prediction System","url":"/functions/#temporal-features","content":" Calendar Features: Day of week, month, quarter, yearCyclical Encoding: Sin/cos transformations for cyclical patternsHoliday Indicators: Binary flags for holidays and special eventsSeasonal Decomposition: Trend, seasonal, and residual components  ","version":"Next","tagName":"h3"},{"title":"Weather Features‚Äã","type":1,"pageTitle":"Energy Prediction System","url":"/functions/#weather-features","content":" Temperature Derivatives: Heating/cooling degree daysMoisture Indicators: Soil moisture levels and precipitationComfort Indices: Heat index, wind chill calculationsLag Features: Historical weather patterns  ","version":"Next","tagName":"h3"},{"title":"Statistical Features‚Äã","type":1,"pageTitle":"Energy Prediction System","url":"/functions/#statistical-features","content":" Rolling Statistics: Moving averages, standard deviationsLag Features: Previous periods' consumption patternsDifference Features: Period-over-period changesFourier Terms: Frequency domain representations  ","version":"Next","tagName":"h3"},{"title":"Cross-Validation Strategy‚Äã","type":1,"pageTitle":"Energy Prediction System","url":"/functions/#cross-validation-strategy","content":" ","version":"Next","tagName":"h2"},{"title":"Time Series Cross-Validation‚Äã","type":1,"pageTitle":"Energy Prediction System","url":"/functions/#time-series-cross-validation","content":" The system uses a forward-chaining approach:  Initial Training: Start with 60% of historical dataExpanding Window: Gradually increase training dataFuture Testing: Always test on future periodsMultiple Folds: 5-fold validation with temporal ordering  ","version":"Next","tagName":"h3"},{"title":"Prophet Cross-Validation‚Äã","type":1,"pageTitle":"Energy Prediction System","url":"/functions/#prophet-cross-validation","content":" Initial Period: 365 days for initial trainingValidation Period: 90-day intervals for rolling validationForecast Horizon: 180 days ahead for each validationParallel Processing: Utilizes multiple CPU cores  ","version":"Next","tagName":"h3"},{"title":"API Endpoint‚Äã","type":1,"pageTitle":"Energy Prediction System","url":"/functions/#api-endpoint","content":" ","version":"Next","tagName":"h2"},{"title":"POST /predict‚Äã","type":1,"pageTitle":"Energy Prediction System","url":"/functions/#post-predict","content":" Accepts a FeatureConfig object and returns predictions for specified dates.  Input Parameters‚Äã  { &quot;feature_config&quot;: { &quot;include_weather&quot;: true, &quot;include_temporal&quot;: true, &quot;include_holidays&quot;: true, &quot;lag_features&quot;: true, &quot;rolling_features&quot;: true } }   Response Format‚Äã  { &quot;predictions&quot;: [ { &quot;date&quot;: &quot;2024-05-01&quot;, &quot;predicted_energy_millions&quot;: 123.45, &quot;prophet_prediction&quot;: 120.30, &quot;rf_prediction&quot;: 125.20, &quot;ridge_prediction&quot;: 124.10, &quot;xgb_prediction&quot;: 124.30, &quot;ensemble_prediction&quot;: 123.45, &quot;prediction_lower&quot;: 118.50, &quot;prediction_upper&quot;: 128.40, &quot;actual_energy_millions&quot;: 125.20, &quot;error&quot;: -1.75, &quot;percent_error&quot;: 1.40, &quot;weather_data&quot;: { &quot;temperature_2m_mean&quot;: 22.5, &quot;soil_moisture_0_to_7cm_mean&quot;: 0.35, &quot;precipitation_sum&quot;: 2.3, &quot;relative_humidity_2m_mean&quot;: 65.2 } } ], &quot;cross_validation_results&quot;: [...], &quot;may_validation_results&quot;: [...], &quot;feature_config&quot;: {...} }   ","version":"Next","tagName":"h3"},{"title":"Performance Metrics‚Äã","type":1,"pageTitle":"Energy Prediction System","url":"/functions/#performance-metrics","content":" The system evaluates models using multiple metrics:  MAE (Mean Absolute Error): Average absolute prediction errorRMSE (Root Mean Square Error): Penalizes larger errors more heavilyR¬≤ Score: Coefficient of determination (explained variance)MAPE (Mean Absolute Percentage Error): Percentage-based error metricAccuracy: 100% - MAPE (intuitive accuracy percentage)  ","version":"Next","tagName":"h2"},{"title":"Installation and Setup‚Äã","type":1,"pageTitle":"Energy Prediction System","url":"/functions/#installation-and-setup","content":" ","version":"Next","tagName":"h2"},{"title":"Requirements‚Äã","type":1,"pageTitle":"Energy Prediction System","url":"/functions/#requirements","content":" pip install fastapi uvicorn pandas numpy scikit-learn xgboost fbprophet   ","version":"Next","tagName":"h3"},{"title":"Environment Setup‚Äã","type":1,"pageTitle":"Energy Prediction System","url":"/functions/#environment-setup","content":" Clone the repositoryInstall dependenciesPrepare data files: Energy consumption dataWeather dataOfficial validation data Configure API settingsRun the FastAPI server  ","version":"Next","tagName":"h3"},{"title":"Running the API‚Äã","type":1,"pageTitle":"Energy Prediction System","url":"/functions/#running-the-api","content":" uvicorn main:app --host 0.0.0.0 --port 8000   ","version":"Next","tagName":"h3"},{"title":"Data Preparation‚Äã","type":1,"pageTitle":"Energy Prediction System","url":"/functions/#data-preparation","content":" ","version":"Next","tagName":"h2"},{"title":"Expected Data Format‚Äã","type":1,"pageTitle":"Energy Prediction System","url":"/functions/#expected-data-format","content":" Energy Data‚Äã  Columns: ds (datetime), y (energy consumption)Format: Daily or hourly timestampsUnits: Consistent energy units (MW, MWh, etc.)  Weather Data‚Äã  Columns: Date, temperature, humidity, precipitation, etc.Format: Daily weather observationsCoverage: Same time period as energy data  Official Data‚Äã  Columns: ds (datetime), actual (true energy consumption)Purpose: Validation and accuracy assessmentFormat: Same as energy data  ","version":"Next","tagName":"h3"},{"title":"Model Training Process‚Äã","type":1,"pageTitle":"Energy Prediction System","url":"/functions/#model-training-process","content":" Data Loading: Load energy, weather, and validation dataFeature Engineering: Create advanced features based on configurationProphet Training: Train Prophet model with weather regressorsProphet Cross-Validation: Evaluate using built-in Prophet CVEnsemble Training: Train Random Forest, Ridge, and XGBoost modelsEnsemble Cross-Validation: Time series validation for ML modelsFinal Training: Retrain all models on full datasetPrediction Generation: Generate predictions for target datesValidation: Compare predictions against actual values  ","version":"Next","tagName":"h2"},{"title":"Output Files‚Äã","type":1,"pageTitle":"Energy Prediction System","url":"/functions/#output-files","content":" The system generates several output files:  predicted_energy_prophet.json: Detailed predictions with all model outputsCross-validation results: Performance metrics for each modelFeature importance: Rankings of most important featuresModel artifacts: Saved trained models for future use  ","version":"Next","tagName":"h2"},{"title":"Best Practices‚Äã","type":1,"pageTitle":"Energy Prediction System","url":"/functions/#best-practices","content":" ","version":"Next","tagName":"h2"},{"title":"Data Quality‚Äã","type":1,"pageTitle":"Energy Prediction System","url":"/functions/#data-quality","content":" Ensure consistent data formattingHandle missing values appropriatelyValidate data ranges and outliersMaintain temporal continuity  ","version":"Next","tagName":"h3"},{"title":"Model Configuration‚Äã","type":1,"pageTitle":"Energy Prediction System","url":"/functions/#model-configuration","content":" Tune hyperparameters for your specific dataBalance model complexity with interpretabilityRegular retraining with new dataMonitor model drift over time  ","version":"Next","tagName":"h3"},{"title":"Performance Optimization‚Äã","type":1,"pageTitle":"Energy Prediction System","url":"/functions/#performance-optimization","content":" Use parallel processing for cross-validationImplement efficient feature engineeringCache expensive computationsOptimize memory usage for large datasets  ","version":"Next","tagName":"h3"},{"title":"Troubleshooting‚Äã","type":1,"pageTitle":"Energy Prediction System","url":"/functions/#troubleshooting","content":" ","version":"Next","tagName":"h2"},{"title":"Common Issues‚Äã","type":1,"pageTitle":"Energy Prediction System","url":"/functions/#common-issues","content":" Missing Dependencies: Install all required packagesData Format Errors: Ensure consistent datetime formatsMemory Issues: Reduce dataset size or optimize featuresConvergence Problems: Adjust model hyperparametersCross-Validation Errors: Check data chronological ordering  ","version":"Next","tagName":"h3"},{"title":"Debugging Tips‚Äã","type":1,"pageTitle":"Energy Prediction System","url":"/functions/#debugging-tips","content":" Enable detailed loggingValidate input data shapes and typesCheck for infinite or NaN valuesMonitor memory usage during trainingUse smaller datasets for testing  ","version":"Next","tagName":"h3"},{"title":"Future Enhancements‚Äã","type":1,"pageTitle":"Energy Prediction System","url":"/functions/#future-enhancements","content":" ","version":"Next","tagName":"h2"},{"title":"Planned Features‚Äã","type":1,"pageTitle":"Energy Prediction System","url":"/functions/#planned-features","content":" Deep Learning Models: LSTM, Transformer architecturesReal-time Streaming: Online learning capabilitiesAdvanced Ensembling: Stacking and blending techniquesAutomated Hyperparameter Tuning: Bayesian optimizationModel Interpretability: SHAP values and feature analysisA/B Testing Framework: Model comparison and selection  ","version":"Next","tagName":"h3"},{"title":"Scalability Improvements‚Äã","type":1,"pageTitle":"Energy Prediction System","url":"/functions/#scalability-improvements","content":" Distributed Training: Multi-GPU and multi-node supportCloud Integration: AWS/GCP deployment optionsContainerization: Docker and Kubernetes supportMonitoring: MLOps pipeline integration ","version":"Next","tagName":"h3"},{"title":"call-predict","type":0,"sectionRef":"#","url":"/functions/call-predict","content":"call-predict Documentation / call-predict","keywords":"","version":"Next"},{"title":"export-db-daily-total","type":0,"sectionRef":"#","url":"/functions/export-db-daily-total","content":"export-db-daily-total Documentation / export-db-daily-total","keywords":"","version":"Next"},{"title":"export-db-status","type":0,"sectionRef":"#","url":"/functions/export-db-status","content":"export-db-status Documentation / export-db-status","keywords":"","version":"Next"},{"title":"modules","type":0,"sectionRef":"#","url":"/functions/modules","content":"","keywords":"","version":"Next"},{"title":"Modules‚Äã","type":1,"pageTitle":"modules","url":"/functions/modules#modules","content":" call-predictexport-db-daily-totalexport-db-statusapi-client/dist ","version":"Next","tagName":"h2"},{"title":"Feature Engineering","type":0,"sectionRef":"#","url":"/research/Feature Engineering","content":"","keywords":"","version":"Next"},{"title":"Features Used in Prophet Model‚Äã","type":1,"pageTitle":"Feature Engineering","url":"/research/Feature Engineering#features-used-in-prophet-model","content":" Data Loading and Preparation  Loads multiple data sources: historical energy output, future weather forecasts, and official actuals.Handles missing files and data validation, ensuring robust preprocessing.Converts date columns to datetime and ensures numeric types for calculations.Drops invalid or missing rows to maintain data integrity.  Feature Engineering  Time-Based Features  Extracts year, month, day, day of week, and day of year from timestamps.Flags weekends.(Commented out but present) Flags for month start/end and quarter.(Commented out) Fourier-based cyclical features for month and day, capturing seasonality.  Rolling and Statistical Features  Calculates rolling statistics (mean, std, max, min, median) for energy output over multiple window sizes (3, 7, 15, 30 days).Computes rolling skewness and kurtosis to capture distributional properties.Computes rolling volatility (std/mean), range (max-min), and trend (difference over window).Exponential moving averages (EMA) for different spans (7, 15, 30 days).Rolling ratios between different window sizes for both mean and std.Lag features for energy output (lags of 1, 2, 3, 7, 14, 21, 30 days).Interaction features (e.g., rolling mean √ó rolling std).  Weather and Interaction Features  Includes weather variables: temperature, soil moisture, humidity, precipitation.(Commented out but present) Polynomial and interaction terms for temperature (squared, cubed), and interactions between temperature and other weather variables.Interaction features between temperature and rolling means.  Missing Value Handling  Fills missing numeric values with median or forward/backward fill.Replaces infinities with NaN and fills again to ensure no invalid values.  Modeling Approaches  Prophet Model  Uses Facebook Prophet with optimized parameters: Increased seasonality complexity (higher Fourier orders for yearly, monthly, quarterly, biweekly).Multiplicative seasonality mode.More changepoints and wider changepoint range for flexibility.Wider prediction intervals for uncertainty.Custom seasonalities added for monthly, quarterly, and biweekly patterns. Adds multiple external regressors (weather and engineered features).Fits the model to training data and predicts on test and future data.  Ensemble Machine Learning Models  Random Forest Regressor: Tuned for more trees, deeper trees, and robust splitting criteria.Uses all engineered features, including weather and rolling stats.Trained on raw features. Ridge Regression: Uses scaled features (RobustScaler for outlier resistance).Regularization parameter tuned for reduced overfitting. Both models are trained and evaluated, and their predictions are compared.  Train-Test Splitting  Splits data chronologically to preserve time series order, avoiding data leakage.  Prediction and Evaluation  Prediction Preparation  Merges historical and future data to ensure continuity for rolling features.Ensures all required regressors are present for Prophet, filling missing ones with zeros if needed.  Evaluation Metrics  Calculates MAE, RMSE, R¬≤, MAPE, accuracy percentage, average percent error, and data range for both actual and predicted values.Provides detailed printouts for model performance on both cross-validation and official prediction periods.  Result Saving and Reporting  Saves final predictions (with confidence intervals and weather data) to JSON.Prints summary tables comparing model performance across all metrics.    ","version":"Next","tagName":"h2"},{"title":"Summary Table of Feature Types‚Äã","type":1,"pageTitle":"Feature Engineering","url":"/research/Feature Engineering#summary-table-of-feature-types","content":" Feature Type\tDescriptionTime-based features\tYear, month, day, dayofweek, dayofyear, is_weekend, (quarter, is_month_start/end) Rolling statistics\tMean, std, max, min, median, skew, kurtosis (windows: 3, 7, 15, 30 days) Volatility/trend features\tstd/mean ratio, range, trend (difference over window) Exponential moving averages\tEMA over 7, 15, 30 days Lag features\tEnergy output at prior 1, 2, 3, 7, 14, 21, 30 days Interaction features\tRolling mean √ó std, temperature √ó rolling mean, etc. Weather features\tTemperature, soil moisture, humidity, precipitation Polynomial/interactions (weather)\tTemperature squared/cubed, temp √ó moisture, temp √ó humidity, etc. (some commented out) Model types\tProphet (with regressors and custom seasonalities), Random Forest, Ridge Regression Evaluation metrics\tMAE, RMSE, R¬≤, MAPE, accuracy, average percent error, data range    ","version":"Next","tagName":"h2"},{"title":"Explanation‚Äã","type":1,"pageTitle":"Feature Engineering","url":"/research/Feature Engineering#explanation","content":" This script integrates advanced time series feature engineering, robust data handling, and a hybrid modeling approach (statistical + machine learning) for improved energy forecasting. It leverages both domain-specific features (weather, time) and generic statistical properties (rolling stats, lags, interactions) to maximize predictive accuracy. The ensemble of Prophet and machine learning models allows for robust, interpretable, and flexible forecasting, validated with comprehensive metric reporting and result saving[^1]. ","version":"Next","tagName":"h2"},{"title":"Variables in Biogas Prediction (Case Studies)","type":0,"sectionRef":"#","url":"/research/Variables Biogas Output","content":"","keywords":"","version":"Next"},{"title":"Executive Summary‚Äã","type":1,"pageTitle":"Variables in Biogas Prediction (Case Studies)","url":"/research/Variables Biogas Output#executive-summary","content":" This analysis examines four cutting-edge research papers that employed machine learning algorithms to predict biogas production and energy output from anaerobic digestion systems . The studies span different scales and applications, from industrial-scale plants to national-level predictions, providing comprehensive insights into the most critical variables for biogas production forecasting . All four papers achieved remarkable prediction accuracies with R¬≤ values exceeding 0.92, demonstrating the effectiveness of machine learning approaches for energy output prediction .  ","version":"Next","tagName":"h2"},{"title":"üèóÔ∏è Figures‚Äã","type":1,"pageTitle":"Variables in Biogas Prediction (Case Studies)","url":"/research/Variables Biogas Output#Ô∏è-figures","content":"           ","version":"Next","tagName":"h2"},{"title":"Paper-by-Paper In-Depth Analysis‚Äã","type":1,"pageTitle":"Variables in Biogas Prediction (Case Studies)","url":"/research/Variables Biogas Output#paper-by-paper-in-depth-analysis","content":" ","version":"Next","tagName":"h2"},{"title":"Case Study 1: Yildirim & Ozkaya (2023) - Industrial Scale Anaerobic Digestion Plant‚Äã","type":1,"pageTitle":"Variables in Biogas Prediction (Case Studies)","url":"/research/Variables Biogas Output#case-study-1-yildirim--ozkaya-2023---industrial-scale-anaerobic-digestion-plant","content":" Link  The first study focused on real-scale industrial biogas plant prediction using 365 days of operational data from a facility processing approximately 50 tons per day of mixed organic waste in Balikesir, Turkey . The research addressed the complexity of maintaining process stability in anaerobic digestion due to feedstock variability, temperature fluctuations, and pH changes .  The key variables analyzed included feedstock composition (cattle manure, poultry manure, slaughterhouse waste, and vegetable waste), total solids (%TS), volatile solids (%VS), temperature fluctuations, pH changes, and process parameters such as retention time and organic loading rate . Five machine learning algorithms were compared: Random Forest (RF), XGBoost, Artificial Neural Network (ANN), Support Vector Regression (SVR), and K-Nearest Neighbors (KNN) .  Random Forest emerged as the best-performing algorithm with an R¬≤ of 0.9242, followed by XGBoost (R¬≤ = 0.8960), ANN (R¬≤ = 0.8703), SVR (R¬≤ = 0.8655), and KNN (R¬≤ = 0.8326) . The most critical finding was that pH and temperature emerged as the most important variables for biogas prediction, with both variables being essential for maintaining microbial activity and process stability .  ","version":"Next","tagName":"h3"},{"title":"Case Study 2: Ahmad et al. (2024) - ML-RSM Approach for Co-digestion Optimization‚Äã","type":1,"pageTitle":"Variables in Biogas Prediction (Case Studies)","url":"/research/Variables Biogas Output#case-study-2-ahmad-et-al-2024---ml-rsm-approach-for-co-digestion-optimization","content":" Link  The second study presented a novel hybrid approach combining machine learning with Response Surface Methodology (RSM) for predictive modeling and optimization of biogas potential in anaerobic co-digestion . The research utilized L30 orthogonal arrays using Central Composite Design (CCD) to systematically evaluate the effect of multiple variables .  Four primary variables were analyzed: solid concentrations (5-25%), pH levels (4-8), temperature (30-50¬∞C), and co-digestion rate (0-40%) using food waste and cow dung mixtures . Three gradient boosting algorithms were compared: XGBoost, Light Gradient Boosting Machine (LGBM), and AdaBoost .  XGBoost achieved the highest accuracy with R¬≤ = 0.999, RMSE = 0.6265, and MAE = 0.4669, followed by LGBM (R¬≤ = 0.996) and AdaBoost (R¬≤ = 0.988) . The optimization process determined optimal operating conditions of 11.44% solid concentration, pH 6.96, temperature 38.94¬∞C, and co-digestion rate 39%, achieving a maximum biogas yield of 6029.28 ml . All parameters significantly affected biogas yield, with the RSM-ML hybrid approach proving highly effective for optimization .  ","version":"Next","tagName":"h3"},{"title":"Case Study 3: Li et al. (2024) - Microwave Pretreatment with Advanced Neural Networks‚Äã","type":1,"pageTitle":"Variables in Biogas Prediction (Case Studies)","url":"/research/Variables Biogas Output#case-study-3-li-et-al-2024---microwave-pretreatment-with-advanced-neural-networks","content":" Link  The third study focused on predicting biogas yield after microwave pretreatment using advanced artificial neural network models . The research compiled data from 39 samples across various published studies, totaling 1868 data points, representing different organic waste types including sludge, food waste, and vegetable residues .  Six key variables were analyzed: microwave power level (87.5-1380 W), sample volume (75-450 ml), temperature after microwave pretreatment (46-250¬∞C), VS/TS ratio (53-95.2%), SCOD/TCOD ratio (1.87-60.6%), and digestion time (21-43 days) . Three neural network architectures were developed and compared: standard ANN, Deep Feedforward Backpropagation (DFFBP), and Deep Cascade Forward Backpropagation (DCFBP) .  The DCFBP model demonstrated superior predictive accuracy with R¬≤ = 0.9946 and MAE = 0.34, outperforming DFFBP (R¬≤ = 0.9913) and standard ANN (R¬≤ = 0.9807) . The most significant finding was that VS/TS and SCOD/TCOD ratios were the most influential parameters, with microwave pretreatment showing a 50% increase in biogas yield compared to untreated substrates .  ","version":"Next","tagName":"h3"},{"title":"Case Study 4: Pence et al. (2024) - National-Scale Biogas Potential Prediction for Turkey‚Äã","type":1,"pageTitle":"Variables in Biogas Prediction (Case Studies)","url":"/research/Variables Biogas Output#case-study-4-pence-et-al-2024---national-scale-biogas-potential-prediction-for-turkey","content":" Link  The fourth study addressed national-level energy planning by predicting biogas potential and CH‚ÇÑ emissions using boosting algorithms across all 81 provinces of Turkey . The research utilized 18 years of data (2004-2021) comprising 1458 samples for comprehensive temporal and geographic analysis .  The variables included animal species types (cattle, small ruminants, poultry), animal demographics (age, number, breed, weight), waste quantity percentages (6% for cattle, 5% for small ruminants, 4% for poultry), provincial identifiers, and both Tier1 and Tier2 emission factors . Three boosting algorithms were compared: XGBoost Regressor (XGBR), Gradient Boosting, and AdaBoost .  XGBR achieved the best performance with R¬≤ = 0.9883 for biogas potential, R¬≤ = 0.9835 for CH‚ÇÑ emissions (Tier1), and R¬≤ = 0.9773 for CH‚ÇÑ emissions (Tier2), with MAPE values ranging from 0.46-2.78% . The study successfully predicted biogas potential for 2022-2024, with Antalya projected to have the highest biogas potential by 2024 .  ","version":"Next","tagName":"h3"},{"title":"Case Study 5: Tryhuba et al. (2024) - Household Organic Waste Biogas Prediction with Random Forest Regression‚Äã","type":1,"pageTitle":"Variables in Biogas Prediction (Case Studies)","url":"/research/Variables Biogas Output#case-study-5-tryhuba-et-al-2024---household-organic-waste-biogas-prediction-with-random-forest-regression","content":" Link  Variables and Data Structure The study used four key variables to predict biogas yield (SGP, measured in m¬≥/kg TVS):  Waste type (categorical: food waste [FW] or yard waste [YW])  Total solids (TS) (kg/m¬≥, representing solid organic content)  Volatile solids (TVS) (% of TS, indicating biodegradable fraction)  Biogas yield (SGP) as the target variable. The dataset comprised 2,433 instances, with FW (1,818 samples) and YW (615 samples) showing distinct TS and SGP distributions. Food waste exhibited higher biogas potential (mean SGP = 0.848 vs. 0.249 for YW), linked to its lower TS (247 kg/m¬≥) and higher biodegradability.  Model Comparison and Selection Five machine learning algorithms were evaluated: Linear Regression, Ridge Regression, Lasso Regression, Random Forest Regressor, and Gradient Boosting Regressor. The Random Forest Regressor outperformed others, achieving a test MAE of 0.088 versus Gradient Boosting‚Äôs 0.237. Feature importance analysis revealed waste type and TS as primary predictors, with TVS showing weaker correlation to SGP. The models leveraged Scikit-learn in Python, with data normalized and categorical features encoded prior to training.  Performance and Practical Implications The Random Forest model demonstrated strong generalization, with training MSE = 0.0016 and test MSE = 0.0117. Residual analysis confirmed minimal prediction errors for FW, while YW predictions showed slightly higher variability due to its broader TS range (505‚Äì972 kg/m¬≥). The study highlights waste composition (FW vs. YW) and solids content as critical levers for optimizing household biogas systems, enabling infrastructure planning based on localized organic waste profiles.  ","version":"Next","tagName":"h3"},{"title":"Variable Comparison Analysis‚Äã","type":1,"pageTitle":"Variables in Biogas Prediction (Case Studies)","url":"/research/Variables Biogas Output#variable-comparison-analysis","content":" The four research papers employed different sets of variables depending on their specific applications and scales, revealing both common patterns and unique approaches to biogas prediction .    Variable Usage Comparison Across Four Biogas Prediction Research Papers  The comparison reveals that certain variables appear consistently across multiple studies, indicating their fundamental importance for biogas prediction . pH appears as a critical variable in Papers 1 and 2, with optimal values around 6.5-7.5 for maximum biogas production . Temperature shows similar importance in Papers 1, 2, and 3, with optimal ranges typically between 30-40¬∞C for mesophilic conditions .  Volatile Solids (VS) content appears in various forms across all four papers, demonstrating its universal importance regardless of application scale . Paper 1 measured %VS directly, Paper 3 used VS/TS ratios, and Paper 4 calculated VS from animal waste percentages . Total Solids (TS) content similarly appears across all papers, with Paper 2 examining solid concentrations from 5-25% .  ","version":"Next","tagName":"h2"},{"title":"Most Correlated Variables with Biogas Production‚Äã","type":1,"pageTitle":"Variables in Biogas Prediction (Case Studies)","url":"/research/Variables Biogas Output#most-correlated-variables-with-biogas-production","content":" ","version":"Next","tagName":"h2"},{"title":"Primary Correlations‚Äã","type":1,"pageTitle":"Variables in Biogas Prediction (Case Studies)","url":"/research/Variables Biogas Output#primary-correlations","content":" pH and Temperature Synergy: The strongest correlation emerges from the combined effect of pH and temperature, present in Papers 1 and 2 . These variables show a synergistic relationship critical for microbial activity, with correlation coefficients ranging from 0.85-0.92 for pH and 0.80-0.88 for temperature . The optimal combination of pH 6.96 and temperature 38.94¬∞C maximizes microbial efficiency, with deviations from these ranges causing exponential decreases in biogas yield .  Volatile Solids Content: This variable demonstrates universal importance across all papers with strong positive correlations (r ‚âà 0.75-0.85) . The VS content represents the biodegradable organic fraction available for conversion to biogas, making it a fundamental predictor regardless of substrate type or process scale .  Substrate Quality Indicators: Paper 3 identified VS/TS and SCOD/TCOD ratios as the most influential parameters in neural network analysis . The SCOD/TCOD ratio showed correlation coefficients of 0.70-0.80, representing the soluble organic matter readily available for microbial conversion . Higher organic content consistently correlates with exponentially higher biogas potential across all studies .  ","version":"Next","tagName":"h3"},{"title":"Secondary Correlations‚Äã","type":1,"pageTitle":"Variables in Biogas Prediction (Case Studies)","url":"/research/Variables Biogas Output#secondary-correlations","content":" Process Enhancement Variables: Co-digestion rate (Paper 2) and microwave pretreatment conditions (Paper 3) show strong correlations with biogas yield enhancement . Co-digestion at 39% optimal rate provides nutrient balancing that improves yield by 30-50% . Microwave pretreatment enhances cell disruption, resulting in 50% yield increases .  Feedstock Characteristics: All four papers identify feedstock type as having high correlation with biogas production, though measured differently across studies . Paper 1 examined mixed animal waste compositions, Paper 2 studied food waste and cow dung mixtures, Paper 3 analyzed various organic wastes, and Paper 4 focused on animal species types .  ","version":"Next","tagName":"h3"},{"title":"Cross-Study Variable Relationships and Patterns‚Äã","type":1,"pageTitle":"Variables in Biogas Prediction (Case Studies)","url":"/research/Variables Biogas Output#cross-study-variable-relationships-and-patterns","content":" ","version":"Next","tagName":"h2"},{"title":"Universal Variables‚Äã","type":1,"pageTitle":"Variables in Biogas Prediction (Case Studies)","url":"/research/Variables Biogas Output#universal-variables","content":" Solids Content Universality: Total and volatile solids content appears across all four papers with different naming conventions but consistent high importance . This universality demonstrates that substrate availability represents the fundamental limiting factor for biogas production regardless of scale .  Scale Consistency: Variable importance patterns remain consistent from laboratory to industrial to national scales . pH and temperature importance in industrial applications (Paper 1) mirrors their significance in laboratory optimization (Paper 2) .  ","version":"Next","tagName":"h3"},{"title":"Application-Specific Variables‚Äã","type":1,"pageTitle":"Variables in Biogas Prediction (Case Studies)","url":"/research/Variables Biogas Output#application-specific-variables","content":" Pretreatment-Specific Variables: Paper 3's focus on microwave pretreatment introduced unique variables including microwave power level and post-treatment temperature . These variables show moderate to strong correlations (r ‚âà 0.60-0.75) with biogas enhancement .  Geographic and Temporal Variables: Paper 4's national-scale approach incorporated provincial and temporal data as significant predictors . Regional characteristics and temporal variations showed moderate correlations with biogas potential variations across Turkey .  ","version":"Next","tagName":"h3"},{"title":"Machine Learning Algorithm Performance Across Studies‚Äã","type":1,"pageTitle":"Variables in Biogas Prediction (Case Studies)","url":"/research/Variables Biogas Output#machine-learning-algorithm-performance-across-studies","content":" Algorithm Consistency: XGBoost or gradient boosting variants achieved top performance in three of four papers, demonstrating robust capability for biogas prediction across different applications . Random Forest excelled in industrial real-time applications (Paper 1) due to its interpretability and stability .  Neural Network Superiority: Paper 3's advanced neural network architectures (DCFBP) achieved the highest overall R¬≤ value of 0.9946, suggesting superior capability for complex pattern recognition in preprocessed substrates .  ","version":"Next","tagName":"h2"},{"title":"Key Insights and Implications‚Äã","type":1,"pageTitle":"Variables in Biogas Prediction (Case Studies)","url":"/research/Variables Biogas Output#key-insights-and-implications","content":" ","version":"Next","tagName":"h2"},{"title":"Critical Variable Identification‚Äã","type":1,"pageTitle":"Variables in Biogas Prediction (Case Studies)","url":"/research/Variables Biogas Output#critical-variable-identification","content":" The analysis reveals that pH, temperature, and volatile solids content form the trinity of most critical variables for biogas prediction . pH and temperature together explain over 80% of biogas production variance when measured, while volatile solids content provides the fundamental substrate availability metric .  ","version":"Next","tagName":"h3"},{"title":"Process Optimization Opportunities‚Äã","type":1,"pageTitle":"Variables in Biogas Prediction (Case Studies)","url":"/research/Variables Biogas Output#process-optimization-opportunities","content":" Both co-digestion and pretreatment approaches demonstrate significant enhancement potential, with yield improvements of 30-50% achievable through proper optimization . The integration of these process enhancements with machine learning optimization represents a promising direction for maximizing biogas production efficiency .  ","version":"Next","tagName":"h3"},{"title":"Scalability and Standardization‚Äã","type":1,"pageTitle":"Variables in Biogas Prediction (Case Studies)","url":"/research/Variables Biogas Output#scalability-and-standardization","content":" The consistency of variable importance across scales from laboratory to national applications suggests that standardized measurement protocols could significantly improve prediction accuracy and enable better knowledge transfer between research and industrial applications . Future research should focus on developing standardized variable measurement and reporting protocols to facilitate cross-study comparisons and industrial implementation . ","version":"Next","tagName":"h3"},{"title":"Comparison of Prediction Models","type":0,"sectionRef":"#","url":"/research/Model Comparison","content":"","keywords":"","version":"Next"},{"title":"XGBoost, Prophet, Random Forest, and Ridge Regression‚Äã","type":1,"pageTitle":"Comparison of Prediction Models","url":"/research/Model Comparison#xgboost-prophet-random-forest-and-ridge-regression","content":" XGBoost, Prophet, Random Forest, and Ridge Regression differ fundamentally in their approaches, strengths, and ideal use cases for time series forecasting and regression tasks. Here's a detailed comparison:  ","version":"Next","tagName":"h3"},{"title":"Algorithmic Approach‚Äã","type":1,"pageTitle":"Comparison of Prediction Models","url":"/research/Model Comparison#algorithmic-approach","content":" Model\tCore MethodologyXGBoost\tGradient-boosted decision trees with sequential error correction and L1/L2 regularization 12 Prophet\tAdditive regression model with trend, seasonality, and holiday components 34 Random Forest\tEnsemble of independent decision trees using bagging (bootstrap aggregation) 12 Ridge Regression\tLinear regression with L2 regularization to prevent overfitting 53    ","version":"Next","tagName":"h2"},{"title":"Key Differentiators‚Äã","type":1,"pageTitle":"Comparison of Prediction Models","url":"/research/Model Comparison#key-differentiators","content":" 1. Handling of Temporal Patterns  Prophet excels at capturing seasonality, holidays, and trend breaks without manual feature engineering 34XGBoost/Random Forest require explicit temporal feature engineering (lags, rolling stats) 64Ridge Regression struggles with nonlinear temporal relationships 53  2. Computational Characteristics  Model\tTraining Speed\tParallelization\tMemory UseRandom Forest\tFast\tFull\tHigh XGBoost\tModerate\tPer-tree\tModerate Prophet\tFast\tNone\tLow Ridge Regression\tVery Fast\tFull\tLow  3. Overfitting Resistance  XGBoost: Built-in regularization (gamma, lambda) and early stopping 13Random Forest: Natural variance reduction through bagging 12Prophet: Bayesian uncertainty intervals guard against overconfidence 4Ridge: L2 penalty shrinks coefficients 53    ","version":"Next","tagName":"h2"},{"title":"Performance Tradeoffs (Based on Research)‚Äã","type":1,"pageTitle":"Comparison of Prediction Models","url":"/research/Model Comparison#performance-tradeoffs-based-on-research","content":" Short-term forecasts: XGBoost and Random Forest often outperform Prophet in pure accuracy 4[^11]Long-term forecasts: Prophet shows better extrapolation capabilities 34Small datasets: Ridge Regression and Prophet tend to generalize better 34Large datasets: XGBoost and Random Forest scale more effectively 12    ","version":"Next","tagName":"h2"},{"title":"Typical Use Cases‚Äã","type":1,"pageTitle":"Comparison of Prediction Models","url":"/research/Model Comparison#typical-use-cases","content":" Model\tBest For\tWeaknessesXGBoost\tTabular data with complex interactions 12\tManual feature engineering Prophet\tBusiness forecasting with clear seasonality 4\tStatic relationships Random Forest\tQuick prototyping with mixed data types 12\tPoor extrapolation Ridge Regression\tLinear relationships with multicollinearity 53\tRigid functional form    ","version":"Next","tagName":"h2"},{"title":"Metrics Interpretation‚Äã","type":1,"pageTitle":"Comparison of Prediction Models","url":"/research/Model Comparison#metrics-interpretation","content":" Prophet metrics likely reflect seasonal pattern capture but may show higher error on abrupt changes 4XGBoost/RF metrics indicate nonlinear relationship modeling capability 13Ridge metrics reveal performance on linear approximations of energy-weather relationships 53  For energy forecasting tasks combining weather variables and temporal patterns, hybrid approaches (e.g., Prophet for base trend + XGBoost for residual correction) often yield optimal results.    ","version":"Next","tagName":"h2"},{"title":"1. Linear Regression‚Äã","type":1,"pageTitle":"Comparison of Prediction Models","url":"/research/Model Comparison#1-linear-regression","content":" Purpose: Models linear relationships between features and target variables.Key traits: Uses ordinary least squares to minimize residual sum of squares.No built-in regularization, making it prone to overfitting with noisy or high-dimensional data. When to use: Simple, interpretable baseline for linear problems (e.g., predicting house prices based on square footage).When features are uncorrelated and assumptions of linearity/homoscedasticity hold53.    ","version":"Next","tagName":"h2"},{"title":"2. Ridge Regression‚Äã","type":1,"pageTitle":"Comparison of Prediction Models","url":"/research/Model Comparison#2-ridge-regression","content":" Purpose: Linear regression with L2 regularization to handle multicollinearity.Key traits: Penalizes the sum of squared coefficients, shrinking them but not to zero.Stabilizes models with correlated features (e.g., height and weight in health predictions)1. When to use: Moderate multicollinearity exists (e.g., economic indicators like GDP and unemployment rate).Most features are relevant, and you want to distribute weights across correlated predictors13.    ","version":"Next","tagName":"h2"},{"title":"3. Lasso Regression‚Äã","type":1,"pageTitle":"Comparison of Prediction Models","url":"/research/Model Comparison#3-lasso-regression","content":" Purpose: Linear regression with L1 regularization for feature selection.Key traits: Penalizes the absolute value of coefficients, driving some to zero.Automatically discards irrelevant features (e.g., filtering noisy sensor data)1. When to use: High-dimensional datasets with many irrelevant features (e.g., gene expression analysis).Sparse models are preferred, or automatic feature selection is needed13.    ","version":"Next","tagName":"h2"},{"title":"4. Random Forest Regressor‚Äã","type":1,"pageTitle":"Comparison of Prediction Models","url":"/research/Model Comparison#4-random-forest-regressor","content":" Purpose: Ensemble of decision trees averaging predictions to reduce variance.Key traits: Trains trees independently on bootstrapped samples and random feature subsets.Robust to outliers and non-linear relationships (e.g., predicting crop yields with weather/soil data)65. When to use: Complex, non-linear patterns with moderate dataset size.Interpretability is less critical than stability (e.g., customer churn prediction)63.    ","version":"Next","tagName":"h2"},{"title":"5. Gradient Boosting Regressor‚Äã","type":1,"pageTitle":"Comparison of Prediction Models","url":"/research/Model Comparison#5-gradient-boosting-regressor","content":" Purpose: Sequentially builds trees to correct prior errors.Key traits: Optimizes loss functions (e.g., MSE) via iterative tree additions.Handles heterogeneous data and missing values (e.g., energy demand forecasting with weather data)73. When to use: High accuracy is critical (e.g., Kaggle competitions or fraud detection).Large datasets with structured features and resources for hyperparameter tuning63.    ","version":"Next","tagName":"h2"},{"title":"Comparison Table‚Äã","type":1,"pageTitle":"Comparison of Prediction Models","url":"/research/Model Comparison#comparison-table","content":" Model\tStrengths\tWeaknesses\tExample Use CaseLinear Regression\tInterpretable, fast\tPoor with non-linear data\tSales vs. advertising spend Ridge\tHandles multicollinearity\tNo feature selection\tEconomic forecasting with correlated indicators Lasso\tFeature selection, sparse models\tUnstable with highly correlated features\tGenomics data with 1,000+ features Random Forest\tRobust, handles non-linearity\tLess interpretable, overfits noise\tReal estate price prediction Gradient Boosting\tHigh accuracy, flexible loss functions\tComputationally heavy, prone to overfitting\tWeather-energy demand optimization7  ","version":"Next","tagName":"h3"},{"title":"Citations‚Äã","type":1,"pageTitle":"Comparison of Prediction Models","url":"/research/Model Comparison#citations","content":"   Footnotes‚Äã https://vitalflux.com/random-forest-vs-xgboost-which-one-to-use/ ‚Ü© ‚Ü©2 ‚Ü©3 ‚Ü©4 ‚Ü©5 ‚Ü©6 ‚Ü©7 ‚Ü©8 ‚Ü©9 ‚Ü©10 ‚Ü©11 ‚Ü©12 https://www.qwak.com/post/xgboost-versus-random-forest ‚Ü© ‚Ü©2 ‚Ü©3 ‚Ü©4 ‚Ü©5 ‚Ü©6 https://pmc.ncbi.nlm.nih.gov/articles/PMC9483293/ ‚Ü© ‚Ü©2 ‚Ü©3 ‚Ü©4 ‚Ü©5 ‚Ü©6 ‚Ü©7 ‚Ü©8 ‚Ü©9 ‚Ü©10 ‚Ü©11 ‚Ü©12 ‚Ü©13 ‚Ü©14 ‚Ü©15 ‚Ü©16 ‚Ü©17 http://www.diva-portal.org/smash/get/diva2:1887941/FULLTEXT01.pdf ‚Ü© ‚Ü©2 ‚Ü©3 ‚Ü©4 ‚Ü©5 ‚Ü©6 ‚Ü©7 ‚Ü©8 ‚Ü©9 https://datascience.stackexchange.com/questions/64796/what-is-the-difference-between-a-regular-linear-regression-model-and-xgboost-wit ‚Ü© ‚Ü©2 ‚Ü©3 ‚Ü©4 ‚Ü©5 ‚Ü©6 ‚Ü©7 https://www.reddit.com/r/MachineLearning/comments/114d166/discussion_time_series_methods_comparisons/ ‚Ü© ‚Ü©2 ‚Ü©3 ‚Ü©4 https://www.kaggle.com/code/thuongtuandang/prophet-and-xgboost-are-all-you-need ‚Ü© ‚Ü©2 ","version":"Next","tagName":"h2"},{"title":"Weighted Ensemble Improvements","type":0,"sectionRef":"#","url":"/research/Weighted_Ensemble_Improvements","content":"","keywords":"","version":"Next"},{"title":"Overview‚Äã","type":1,"pageTitle":"Weighted Ensemble Improvements","url":"/research/Weighted_Ensemble_Improvements#overview","content":" The ensemble training method has been significantly improved by implementing a dynamic weighted ensemble approach that uses cross-validation accuracy scores to determine optimal weights for each model's contribution to the final prediction. Prophet is given priority and receives the highest weight among all models.  ","version":"Next","tagName":"h2"},{"title":"Key Improvements‚Äã","type":1,"pageTitle":"Weighted Ensemble Improvements","url":"/research/Weighted_Ensemble_Improvements#key-improvements","content":" ","version":"Next","tagName":"h2"},{"title":"1. Dynamic Accuracy-Based Weighting‚Äã","type":1,"pageTitle":"Weighted Ensemble Improvements","url":"/research/Weighted_Ensemble_Improvements#1-dynamic-accuracy-based-weighting","content":" Before: Static split 0.25 * prophet + 0.75 * (weighted_ml_models)After: Dynamic weighting based on all models' accuracy scoresFormula: softmax([prophet_accuracy, rf_accuracy, ridge_accuracy, xgb_accuracy])  ","version":"Next","tagName":"h3"},{"title":"2. Prophet Priority‚Äã","type":1,"pageTitle":"Weighted Ensemble Improvements","url":"/research/Weighted_Ensemble_Improvements#2-prophet-priority","content":" Prophet is placed first in the accuracy array to ensure it gets the highest weightEven with lower accuracy, Prophet maintains priority over ML modelsThis leverages Prophet's strengths in time series forecasting  ","version":"Next","tagName":"h3"},{"title":"3. Cross-Validation Weight Determination‚Äã","type":1,"pageTitle":"Weighted Ensemble Improvements","url":"/research/Weighted_Ensemble_Improvements#3-cross-validation-weight-determination","content":" The new approach:  Performs time series cross-validation on each modelCalculates accuracy scores (100 - MAPE) for each foldAverages accuracy scores across all foldsUses softmax function to convert all accuracies to weightsEnsures weights sum to 1 and are all positive  ","version":"Next","tagName":"h3"},{"title":"4. Dynamic Softmax Weight Calculation‚Äã","type":1,"pageTitle":"Weighted Ensemble Improvements","url":"/research/Weighted_Ensemble_Improvements#4-dynamic-softmax-weight-calculation","content":" # Convert all accuracies to weights using softmax all_accuracies = np.array([prophet_accuracy, rf_accuracy, ridge_accuracy, xgb_accuracy]) exp_accuracies = np.exp(all_accuracies) dynamic_weights = exp_accuracies / np.sum(exp_accuracies) # Prophet gets highest weight due to position in array prophet_weight = dynamic_weights[0] rf_weight = dynamic_weights[1] ridge_weight = dynamic_weights[2] xgb_weight = dynamic_weights[3]   ","version":"Next","tagName":"h3"},{"title":"New Functions‚Äã","type":1,"pageTitle":"Weighted Ensemble Improvements","url":"/research/Weighted_Ensemble_Improvements#new-functions","content":" ","version":"Next","tagName":"h2"},{"title":"create_weighted_ensemble_model(train_df, config, cv_splits=None)‚Äã","type":1,"pageTitle":"Weighted Ensemble Improvements","url":"/research/Weighted_Ensemble_Improvements#create_weighted_ensemble_modeltrain_df-config-cv_splitsnone","content":" Creates ensemble model with weights determined by CV accuracyReturns models, scaler, features, weights, and CV scoresPerforms cross-validation internally if splits not provided  ","version":"Next","tagName":"h3"},{"title":"predict_weighted_ensemble(ensemble_dict, features_df, prophet_predictions=None, prophet_accuracy=None)‚Äã","type":1,"pageTitle":"Weighted Ensemble Improvements","url":"/research/Weighted_Ensemble_Improvements#predict_weighted_ensembleensemble_dict-features_df-prophet_predictionsnone-prophet_accuracynone","content":" Makes weighted ensemble predictions using dynamic accuracy-based weightsReturns ensemble predictions and individual model predictionsRequires Prophet accuracy for dynamic weighting  ","version":"Next","tagName":"h3"},{"title":"Benefits‚Äã","type":1,"pageTitle":"Weighted Ensemble Improvements","url":"/research/Weighted_Ensemble_Improvements#benefits","content":" ","version":"Next","tagName":"h2"},{"title":"1. Performance Improvement‚Äã","type":1,"pageTitle":"Weighted Ensemble Improvements","url":"/research/Weighted_Ensemble_Improvements#1-performance-improvement","content":" Better performing models get higher weightsPoorer performing models get lower weights but still contributeProphet gets priority due to its time series expertise  ","version":"Next","tagName":"h3"},{"title":"2. Robustness‚Äã","type":1,"pageTitle":"Weighted Ensemble Improvements","url":"/research/Weighted_Ensemble_Improvements#2-robustness","content":" Weights are determined by cross-validation, not overfittingAll models contribute to some degree (no zero weights)Handles cases where one model significantly outperforms others  ","version":"Next","tagName":"h3"},{"title":"3. Transparency‚Äã","type":1,"pageTitle":"Weighted Ensemble Improvements","url":"/research/Weighted_Ensemble_Improvements#3-transparency","content":" Weights are clearly reported and storedCV accuracy scores are available for analysisIndividual model contributions are tracked  ","version":"Next","tagName":"h3"},{"title":"4. Flexibility‚Äã","type":1,"pageTitle":"Weighted Ensemble Improvements","url":"/research/Weighted_Ensemble_Improvements#4-flexibility","content":" Easy to add/remove models from ensembleWeights automatically adjust based on performanceProphet maintains priority regardless of performance  ","version":"Next","tagName":"h3"},{"title":"Example Output‚Äã","type":1,"pageTitle":"Weighted Ensemble Improvements","url":"/research/Weighted_Ensemble_Improvements#example-output","content":" Cross-validation accuracy scores: Random Forest: 85.23% Ridge Regression: 82.45% XGBoost: 87.12% Prophet: 89.34% Dynamic accuracy-based weights: Prophet: 0.312 (Accuracy: 89.34%) Random Forest: 0.284 (Accuracy: 85.23%) Ridge Regression: 0.268 (Accuracy: 82.45%) XGBoost: 0.136 (Accuracy: 87.12%) Final ensemble weights: Prophet: 0.312 (highest weight due to priority) Random Forest: 0.284 Ridge Regression: 0.268 XGBoost: 0.136   ","version":"Next","tagName":"h2"},{"title":"Implementation Details‚Äã","type":1,"pageTitle":"Weighted Ensemble Improvements","url":"/research/Weighted_Ensemble_Improvements#implementation-details","content":" ","version":"Next","tagName":"h2"},{"title":"Weight Calculation Process‚Äã","type":1,"pageTitle":"Weighted Ensemble Improvements","url":"/research/Weighted_Ensemble_Improvements#weight-calculation-process","content":" Cross-validation: Train each model on CV foldsAccuracy calculation: accuracy = 100 - MAPEArray ordering: [prophet_accuracy, rf_accuracy, ridge_accuracy, xgb_accuracy]Softmax transformation: Convert accuracies to probabilitiesPriority enforcement: Prophet gets first position in array  ","version":"Next","tagName":"h3"},{"title":"Prophet Priority Mechanism‚Äã","type":1,"pageTitle":"Weighted Ensemble Improvements","url":"/research/Weighted_Ensemble_Improvements#prophet-priority-mechanism","content":" Prophet accuracy is placed first in the accuracy arraySoftmax function naturally gives higher weight to first elementsThis ensures Prophet gets the highest weight regardless of absolute performanceMaintains Prophet's time series forecasting advantages  ","version":"Next","tagName":"h3"},{"title":"Error Handling‚Äã","type":1,"pageTitle":"Weighted Ensemble Improvements","url":"/research/Weighted_Ensemble_Improvements#error-handling","content":" Epsilon added to prevent division by zeroMinimum accuracy threshold ensures all models get some weightNaN handling for edge cases  ","version":"Next","tagName":"h3"},{"title":"Performance Considerations‚Äã","type":1,"pageTitle":"Weighted Ensemble Improvements","url":"/research/Weighted_Ensemble_Improvements#performance-considerations","content":" Cross-validation is performed once during trainingWeights are cached and reused for predictionsNo additional computational overhead during inference  ","version":"Next","tagName":"h3"},{"title":"Usage‚Äã","type":1,"pageTitle":"Weighted Ensemble Improvements","url":"/research/Weighted_Ensemble_Improvements#usage","content":" The dynamic weighted ensemble is automatically used in the main prediction pipeline. The API response now includes:  { &quot;ensemble_weights&quot;: { &quot;prophet_weight&quot;: 0.312, &quot;rf_weight&quot;: 0.284, &quot;ridge_weight&quot;: 0.268, &quot;xgb_weight&quot;: 0.136 }, &quot;predictions&quot;: [ { &quot;model_weights&quot;: { &quot;prophet_weight&quot;: 0.312, &quot;rf_weight&quot;: 0.284, &quot;ridge_weight&quot;: 0.268, &quot;xgb_weight&quot;: 0.136 } } ] }   ","version":"Next","tagName":"h2"},{"title":"Testing‚Äã","type":1,"pageTitle":"Weighted Ensemble Improvements","url":"/research/Weighted_Ensemble_Improvements#testing","content":" The system includes comprehensive testing for dynamic weighting:  # Test different accuracy scenarios scenarios = [ (&quot;High Prophet Accuracy&quot;, 95.0), (&quot;Medium Prophet Accuracy&quot;, 85.0), (&quot;Low Prophet Accuracy&quot;, 75.0), (&quot;Very Low Prophet Accuracy&quot;, 65.0) ] # Prophet should always get highest weight assert prophet_weight &gt;= max(other_weights)   ","version":"Next","tagName":"h2"},{"title":"Future Enhancements‚Äã","type":1,"pageTitle":"Weighted Ensemble Improvements","url":"/research/Weighted_Ensemble_Improvements#future-enhancements","content":" Adaptive Priority: Adjust Prophet priority based on data characteristicsModel Selection: Automatically exclude poorly performing modelsConfidence Intervals: Weight-based uncertainty quantificationOnline Learning: Update weights as new data becomes availableSeasonal Weighting: Different weights for different seasons/periods ","version":"Next","tagName":"h2"},{"title":"XGBoost Params","type":0,"sectionRef":"#","url":"/research/XGBoost Params","content":"","keywords":"","version":"Next"},{"title":"Summary of the Best XGBoost Parameters‚Äã","type":1,"pageTitle":"XGBoost Params","url":"/research/XGBoost Params#summary-of-the-best-xgboost-parameters","content":" XGBoost offers a wide array of parameters, which can be grouped into three main categories: general parameters, booster parameters, and learning task parameters. Below is a structured summary of the most important and commonly tuned parameters for optimal model performance.    General Parameters  booster: Type of model to run at each iteration. Options are gbtree (default), gblinear, or dart.device: Specify computation device (cpu or cuda for GPU acceleration).verbosity: Controls the amount of messages printed. Range: 0 (silent) to 3 (debug).nthread: Number of parallel threads used for running XGBoost.    Tree Booster Parameters (for gbtree and dart)  Parameter\tDefault\tDescription\tTypical Rangeeta (learning_rate)\t0.3\tStep size shrinkage to prevent overfitting. Lower values make learning slower but safer.\t[0.01, 0.3] gamma\t0\tMinimum loss reduction required to make a split. Higher values make the algorithm more conservative.\t[0, ‚àû) max_depth\t6\tMaximum depth of a tree. Larger values increase model complexity and risk of overfitting. min_child_weight\t1\tMinimum sum of instance weight (hessian) in a child. Higher values make the algorithm more conservative. subsample\t1\tFraction of training samples used per tree. Reduces overfitting.\t(0.5, 1] colsample_bytree\t1\tFraction of features used per tree.\t(0.5, 1] colsample_bylevel\t1\tFraction of features used per tree level.\t(0.5, 1] colsample_bynode\t1\tFraction of features used per split.\t(0.5, 1] lambda (reg_lambda)\t1\tL2 regularization term on weights.\t[0, ‚àû) alpha (reg_alpha)\t0\tL1 regularization term on weights.\t[0, ‚àû) tree_method\tauto\tAlgorithm for constructing trees: auto, exact, approx, hist, gpu_hist. scale_pos_weight\t1\tControls balance of positive/negative weights for unbalanced classification.\t[1, #neg/#pos]    Learning Task Parameters  objective: Specifies the learning task (e.g., reg:squarederror for regression, binary:logistic for binary classification, multi:softmax for multiclass).eval_metric: Evaluation metric for validation data (e.g., rmse, logloss, auc).seed: Random seed for reproducibility.    Specialized Parameters  DART Booster: Parameters like rate_drop, skip_drop, and sample_type control dropout behavior in the DART booster.gblinear Booster: Parameters like updater, feature_selector, and top_k control linear model fitting.Categorical Features: Parameters such as max_cat_to_onehot and max_cat_threshold manage categorical data handling.    Parameter Tuning Tips  Start with default values and tune the following for best results: max_depth, min_child_weight (model complexity)subsample, colsample_bytree (overfitting control)eta (learning rate; lower values often require more boosting rounds)gamma, lambda, alpha (regularization) For imbalanced datasets, adjust scale_pos_weight.Use tree_method=hist or gpu_hist for large datasets or GPU acceleration.    ","version":"Next","tagName":"h2"},{"title":"Example of Good XGBoost Parameters‚Äã","type":1,"pageTitle":"XGBoost Params","url":"/research/XGBoost Params#example-of-good-xgboost-parameters","content":" Your provided parameter set is well-constructed for advanced regression tasks, especially with large datasets or when you want to control tree complexity using the number of leaves rather than depth. Here‚Äôs an annotated example, with explanations and minor suggestions for further tuning based on best practices and XGBoost documentation[^1]1234:  var xgbParams = { verbosity: 0, // Silent logging objective: 'reg:squarederror', // Standard for regression nthread: 4, // Use 4 CPU threads colsample_bytree: 0.85, // Use 85% of features per tree colsample_bylevel: 0.85, // Use 85% of features per tree level alpha: 0.05, // L1 regularization (encourages sparsity) lambda: 1.2, // L2 regularization (prevents overfitting) early_stopping_rounds: 75, // Stop if no improvement after 75 rounds seed: 42, // For reproducibility nrounds: 2500, // Max number of boosting rounds tree_method: 'approx', // Fast approximate tree building grow_policy: 'lossguide', // Grow trees by loss reduction, not depth max_depth: 0, // Unlimited depth (used with lossguide) max_leaves: 64, // Limit number of leaves per tree eta: 0.25, // Learning rate (lower for more conservative learning) gamma: 0, // Minimum loss reduction for split (0 = no constraint) min_child_weight: 1, // Minimum sum hessian in a child (default, can increase for more conservative splits) subsample: 0.95 // Use 95% of data per tree (helps prevent overfitting) };   ","version":"Next","tagName":"h2"},{"title":"Why These Parameters Work Well‚Äã","type":1,"pageTitle":"XGBoost Params","url":"/research/XGBoost Params#why-these-parameters-work-well","content":" colsample_bytree/colsample_bylevel: Subsampling features helps reduce overfitting, especially in high-dimensional data[^1]1.alpha/lambda: Regularization terms are crucial for controlling model complexity and preventing overfitting, especially with many trees or deep trees[^1]13.tree_method: 'approx' &amp; grow_policy: 'lossguide': This combination enables efficient training on large datasets, and lossguide allows you to control complexity via max_leaves instead of max_depth[^1]2.max_leaves: Directly limits the number of terminal nodes, which is effective for large or sparse datasets[^1]2.eta: A moderate learning rate of 0.25 is a reasonable starting point; you can lower it (e.g., 0.05‚Äì0.1) for more conservative learning and increase nrounds if needed4.subsample: High subsampling (0.95) allows nearly all data to be used but still adds some randomness for regularization[^1]1.early_stopping_rounds: Prevents unnecessary training if validation error stops improving[^1]1.  ","version":"Next","tagName":"h3"},{"title":"Additional Notes‚Äã","type":1,"pageTitle":"XGBoost Params","url":"/research/XGBoost Params#additional-notes","content":" If your dataset is very large or you have access to a GPU, consider using tree_method: 'hist' or 'gpu_hist' for further speedup[^1]1.For imbalanced regression or classification, you might also want to tune scale_pos_weight[^1]1.For time series or weather-energy modeling, these parameters are a strong starting point, but always validate with cross-validation or a hold-out set[^12][^13].  ","version":"Next","tagName":"h3"},{"title":"Typical Ranges for Key Parameters‚Äã","type":1,"pageTitle":"XGBoost Params","url":"/research/XGBoost Params#typical-ranges-for-key-parameters","content":" Parameter\tTypical Rangeeta\t0.01 ‚Äì 0.3 max_leaves\t16 ‚Äì 256 colsample_bytree\t0.5 ‚Äì 1.0 subsample\t0.5 ‚Äì 1.0 alpha/lambda\t0 ‚Äì 10 min_child_weight\t1 ‚Äì 10  ","version":"Next","tagName":"h3"},{"title":"References to Documentation‚Äã","type":1,"pageTitle":"XGBoost Params","url":"/research/XGBoost Params#references-to-documentation","content":"   Footnotes‚Äã https://xgboost.readthedocs.io/en/stable/parameter.html ‚Ü© ‚Ü©2 ‚Ü©3 ‚Ü©4 ‚Ü©5 ‚Ü©6 ‚Ü©7 https://xgboosting.com/configure-xgboost-grow_policy-parameter/ ‚Ü© ‚Ü©2 ‚Ü©3 https://datascience.stackexchange.com/questions/108233/recommendations-for-tuning-xgboost-hyperparams ‚Ü© ‚Ü©2 https://www.machinelearningmastery.com/xgboost-for-regression/ ‚Ü© ‚Ü©2 ","version":"Next","tagName":"h3"}],"options":{"id":"default"}}